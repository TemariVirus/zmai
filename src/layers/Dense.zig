//! A dense (i.e., fully connected) layer with a fixed activation function, and
//! trainable weights and biases.

const std = @import("std");
const Allocator = std.mem.Allocator;
const assert = std.debug.assert;

const root = @import("../root.zig");
const activations = root.activations;
const Activation = activations.Activation;
const Layer = root.layers.Layer;

const Self = @This();

/// The weights of the layer. The nth `inputSize()` elements are the weights
/// of the connections between the nth output neuron and the input neurons.
weights: []f32,
biases: []f32,
activation: Activation,

/// Initializes a dense layer. The initial parameters are generated by calling
/// `rand_fn`.
pub fn init(allocator: Allocator, input_size: usize, output_size: usize, activation: Activation, rand_fn: *const fn () f32) !Self {
    const weights = try allocator.alloc(f32, output_size * input_size);
    for (weights) |*w| {
        w.* = rand_fn();
    }

    const biases = try allocator.alloc(f32, output_size);
    for (biases) |*b| {
        b.* = rand_fn();
    }

    return .{
        .weights = weights,
        .biases = biases,
        .activation = activation,
    };
}

/// Frees the memory used by this layer. `allocator` is the same allocator that
/// was passed in to `init`.
pub fn deinit(self: Self, allocator: Allocator) void {
    allocator.free(self.weights);
    allocator.free(self.biases);
}

pub fn inputSize(self: Self) usize {
    return self.weights.len / self.biases.len;
}

pub fn outputSize(self: Self) usize {
    return self.biases.len;
}

pub fn size(self: Self) usize {
    return self.weights.len + self.biases.len;
}

pub fn forward(self: Self, input: []const f32, output: []f32) void {
    assert(input.len == self.inputSize());
    assert(output.len == self.outputSize());

    @memcpy(output, self.biases);

    var k: usize = 0;
    for (0..self.outputSize()) |i| {
        for (0..self.inputSize()) |j| {
            output[i] += input[j] * self.weights[k];
            k += 1;
        }
    }

    activations.forward(self.activation, output);
}

pub fn backward(
    self: Self,
    input: []f32,
    output: []f32,
    output_grad: []const f32,
    deltas: []f32,
) void {
    assert(input.len == self.inputSize());
    assert(output.len == self.outputSize());
    assert(output_grad.len == self.outputSize());
    assert(deltas.len == self.weights.len + self.biases.len);

    activations.backward(self.activation, output);

    var k: usize = 0;
    for (0..output.len) |i| {
        const grad = output[i] * output_grad[i];
        deltas[self.weights.len + i] -= grad;
        for (0..input.len) |j| {
            deltas[k] -= grad * input[j];
            k += 1;
        }
    }

    k = 0;
    for (0..input.len) |i| {
        input[i] = 0.0;
    }
    for (0..output.len) |i| {
        for (0..input.len) |j| {
            input[j] += output[i] * output_grad[i] * self.weights[k];
            k += 1;
        }
    }
}

/// Updates the weights and biases of this layer.
pub fn update(self: Self, deltas: []const f32, learning_rate: f32) void {
    assert(deltas.len == self.weights.len + self.biases.len);

    for (0..self.weights.len) |i| {
        self.weights[i] += learning_rate * deltas[i];
    }
    for (0..self.biases.len) |i| {
        self.biases[i] += learning_rate * deltas[self.weights.len + i];
    }
}
